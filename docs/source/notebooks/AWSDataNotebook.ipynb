{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_THIS NOTEBOOK IS NOT CURRENTLY WORKING (due to issues with the AWS dtata)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using data from AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A significant amount of Earth System Model (ESM) data is publicly available online, including data from the CESM Large Ensemble, CMIP5, and CMIP6 datasets. For accessing a single file, we can specify the file (typically netcdf or zarr format) and its location and then use fsspec (the \"Filesystem Spec+  python package) and xarray  to create a array.dataset.  For several files, the intake_esm python module (https://github.com/intake/intake-esm) is particularly nice for obtaining the data and put it into an xarray.dataset.\n",
    "\n",
    "This notebook assumes familiarity with the Tutorial Notebook.  It additionally shows how to gather data from an ESM collection, put it into a dataset, and then create simple plots using the data with ldcpy.\n",
    "\n",
    "#### Example Data\n",
    "\n",
    "The example data we use is from the CESM Large Ensemble, member 31. This ensemble data has been lossily compressed and reconstructed as part of a blind evaluation study of lossy data compression in LENS (e.g., http://www.cesm.ucar.edu/projects/community-projects/LENS/projects/lossy-data-compression.html or https://gmd.copernicus.org/articles/9/4381/2016/).\n",
    "\n",
    "Most of the data from the CESM Large Ensemble Project has been made available on Amazon Web Services (Amazon S3), see http://ncar-aws-www.s3-website-us-west-2.amazonaws.com/CESM_LENS_on_AWS.htm .\n",
    "\n",
    "For comparison purposes, the original (non-compressed) data for Ensemble 31 has recently been made available on Amazon Web Services (Amazon S3)  in the \"ncar-cesm-lens-baker-lossy-compression-test\" bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ldcpy root to system path\n",
    "import sys\n",
    "sys.path.insert(0,'../../../')\n",
    "\n",
    "# Import ldcpy package\n",
    "# Autoreloads package everytime the package is called, so changes to code will be reflected in the notebook if the above sys.path.insert(...) line is uncommented.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import ldcpy\n",
    "\n",
    "import intake\n",
    "import fsspec\n",
    "import xarray as xr\n",
    "\n",
    "#display the plots in this notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: using fsspec and xr.open_zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, specify the filesystem and location of the data.  Here we are accessing the original data from CESM-LENS ensemble 31, which is available on Amazon S3 in the store named _\"ncar-cesm-lens-baker-lossy-compression-test\"_ bucket.\n",
    "\n",
    "First we listing all available files (which are timeseries files containing a single variable) for that dataset. Note that unlike in the TutorialNotebook (which used NetCDF files), these files are all zarr format. Both monthly and daily data is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = fsspec.filesystem('s3', anon=True)\n",
    "stores = fs.ls(\"ncar-cesm-lens-baker-lossy-compression-test/lens-ens31/\")[1:]\n",
    "stores[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we select the file from the store that we want and open it as an xarray.Dataset using xr.open_zarr(). Here we grab data for the first 2D daily variable, FLNS (net longwave flux at surface, in $W/m^2$), in the list (accessed by it location -- stores[0])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = fs.get_mapper(stores[0])\n",
    "ds_flns = xr.open_zarr(store, consolidated=True, decode_times=False)\n",
    "ds_flns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above returned an xarray.Dataset.\n",
    "\n",
    "Now let's grab the TMQ (Total vertically integrated precipitatable water) and the TS (surface temperature data) and PRECT (precipitation rate) data from AWS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TMQ data\n",
    "store2 = fs.get_mapper(stores[16])\n",
    "ds_tmq = xr.open_zarr(store2, consolidated=True, decode_times=False)\n",
    "#TS data\n",
    "store3 = fs.get_mapper(stores[20])\n",
    "ds_ts = xr.open_zarr(store3, consolidated=True, decode_times=False)\n",
    "#PRECT data\n",
    "store4 = fs.get_mapper(stores[11])\n",
    "ds_prect = xr.open_zarr(store4, consolidated=True, decode_times=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the original data for FLNS and TMQ and TS and PRECT. Next we want to get the lossy compressed variants to compare with these. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Using intake_esm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will demonstrate using the intake_esm module to get the lossy variants of the files retrieved above.  We can use the intake_esm module to search for and open several files as xarray.Dataset objects. The code below is modified from the intake_esm documentation, available here: https://intake-esm.readthedocs.io/en/latest/?badge=latest#overview."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to use ensemble 31 data from the CESM-LENS collection on AWS, which (as explained above) has been subjected to lossy compression. Many catalogs for publicly available datasets are accessible via intake-esm can be found at https://github.com/NCAR/intake-esm-datastore/tree/master/catalogs, including for CESM-LENS. We can open that collection as follows (see here: https://github.com/NCAR/esm-collection-spec/blob/master/collection-spec/collection-spec.md#attribute-object): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_loc = \"https://raw.githubusercontent.com/NCAR/cesm-lens-aws/master/intake-catalogs/aws-cesm1-le.json\"\n",
    "aws_col = intake.open_esm_datastore(aws_loc)\n",
    "aws_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we search for the subset of the collection (dataset and variables) that we are interested in.  Let's grab FLNS, TMQ, and TS daily data from the atm component for our comparison (available data in this collection is listed here: http://ncar-aws-www.s3-website-us-west-2.amazonaws.com/CESM_LENS_on_AWS.htm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we want daily data for FLNS, TMQ, and TS\n",
    "aws_col_subset = aws_col.search(component='atm', frequency='daily', experiment='20C',\n",
    "                        variable=[\"FLNS\", \"TS\", \"TMQ\"])\n",
    "#display header info to verify that we got the right variables\n",
    "aws_col_subset.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we load matching catalog entries into xarray datasets (https://intake-esm.readthedocs.io/en/latest/api.html#intake_esm.core.esm_datastore.to_dataset_dict).  We create a dictionary of datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_dict = aws_col_subset.to_dataset_dict(zarr_kwargs={\"consolidated\": True, \"decode_times\": True}, \n",
    "                                   storage_options={\"anon\": True}, cdf_kwargs={'chunks': {}, 'decode_times': False})\n",
    "dset_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the dataset keys to ensure that what we want is present.  Here we only have oneentry in the dictonary as we requested the same time period and output frequency for all variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, put the dataset that we are interested from the dictionary into its own dataset variable. (We want the 20th century daily data -- which is our only option.) \n",
    "\n",
    "Also note from above that there are 40 ensemble members - and we just want ensemble 31 (member_id = 30 as can be seen in the coordinates above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_ds=(dset_dict['atm.20C.daily'])\n",
    "aws_ds = aws_ds.isel(member_id=30)\n",
    "aws_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have datasets for the original and the lossy compressed data for FLNS, TMQ, and TS, which we can extract into a dataset for each variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the three variables from aws_ds as datasets\n",
    "aws_flns = aws_ds['FLNS'].to_dataset()\n",
    "aws_tmq = aws_ds['TMQ'].to_dataset()\n",
    "aws_ts = aws_ds['TS'].to_dataset()\n",
    "aws_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## why can't I get PRECT?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we want daily data for PRECT\n",
    "aws_col_subset2 = aws_col.search(component='atm', frequency='daily', experiment='20C',\n",
    "                        variable=[\"PRECT\"])\n",
    "aws_col_subset = aws_col.search(component='atm', frequency='daily', experiment='20C',\n",
    "                        variable=[\"FLNS\", \"TS\", \"TMQ\"])\n",
    "#display header info to verify that we got the right variables\n",
    "aws_col_subset2.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_dict = aws_col_subset.to_dataset_dict(zarr_kwargs={\"consolidated\": True, \"decode_times\": True}, \n",
    "                                   storage_options={\"anon\": True})\n",
    "dset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_dict2 = aws_col_subset2.to_dataset_dict(zarr_kwargs={\"consolidated\":True, \"decode_times\": True}, \n",
    "                                   storage_options={\"anon\": True})\n",
    "\n",
    "dset_dict2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use _ldcpy_ to compare the original data to the lossy compressed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use ldcpy, we need to group the data that we want to compare (like variables) into dataset collections.  In the Tutorial Notebook, we used ldcpy.open_datasets() to do this as we needed to get the data from the NetCDF files.  Here we already loaded the data from AWS into datasets, so we just need to use ldcpy.collect_datasets() to form collections of the datasets that we want to compare.\n",
    "\n",
    "ldcpy.collect_datasets() requires the following three arguments:\n",
    "\n",
    "- _varnames_ : the variable(s) of interest to combine across files (typically the timeseries file variable name)\n",
    "- _list_of_ds_ : a list of the xarray datasets\n",
    "- _labels_ : a corresponding list of names (or labels) for each dataset in the collection\n",
    "\n",
    "Note: This function is a wrapper for xarray.concat(), and any additional key/value pairs passed in as a dictionary are used as arguments to xarray.concat(). \n",
    "\n",
    "We will create 3 collections for ldcpy (one each for FLNS, TMQ, and TS) and assign labels \"original\" and \"lossy\" to the respective datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FLNS collection\n",
    "col_flns = ldcpy.collect_datasets([\"FLNS\"], [ds_flns, aws_flns], ['original', 'lossy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_flns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why is the time slice number so large?  IT is starting in 1850 instead of 1920???***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TMQ collection\n",
    "col_tmq = ldcpy.collect_datasets([\"TMQ\"], [ds_tmq, aws_tmq], ['original', 'lossy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ts collection\n",
    "col_ts = ldcpy.collect_datasets([\"TS\"], [ds_ts, aws_ts], ['original', 'lossy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our collections, let's make some plots!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparison between mean FLNS values in col_flns \"original\" and \"lossy\" datasets\n",
    "ldcpy.plot(col_flns, \"FLNS\", sets=['original', 'lossy'], metric='mean', calc_ssim = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of mean winter TS values in the col_ts \"original\" and \"lossy\" datasets\n",
    "ldcpy.plot(col_ds, \"TS\", sets=['original','lossy'], metric='mean', subset=\"winter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make your own catalog for intake-esm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original data for FLNS and TMQ and TS and PRECT (from Amazon S3 in the \"ncar-cesm-lens-baker-lossy-compression-test\" bucket)  was loaded above using method 1.  An alternative would be to create our own catalog for this data for use with intake-esm.  To illustrate this, we created a test_catalog.csv and test_collection.json file for this particular simple example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first open our collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_col_loc = \"./collections/test_collection.json\"\n",
    "my_col = intake.open_esm_datastore(my_col_loc)\n",
    "my_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing the head() gives us the file names\n",
    "my_col.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load all of these into our dictionary! (So we don't need to do the search to make a subset of variables as above in Method 2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dset_dict = my_col.to_dataset_dict(zarr_kwargs={\"consolidated\": True, \"decode_times\": True}, \n",
    "                                   storage_options={\"anon\": True})\n",
    "my_dset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we again just want the 20th century daily data\n",
    "my_ds=(my_dset_dict['atm.20C.daily'])\n",
    "my_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can make a dataset for each variable as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_ts = my_ds[\"TS\"].to_dataset()\n",
    "my_tmq = my_ds[\"TMQ\"].to_dataset()\n",
    "my_prect = my_ds[\"PRECT\"].to_dataset()\n",
    "my_flns = my_ds[\"FLNS\"].to_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can form new collections and do comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
