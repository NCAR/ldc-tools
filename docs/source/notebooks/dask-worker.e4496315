distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.7.215:42597'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.7.215:46481'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.7.215:40401'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.7.215:41159'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.7.215:42545'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.7.215:34669'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.7.215:42819'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.7.215:33145'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.7.215:34697'
distributed.diskutils - INFO - Found stale lock file and directory '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-4qx0x71p', purging
distributed.diskutils - INFO - Found stale lock file and directory '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-jx8aj7_7', purging
distributed.diskutils - INFO - Found stale lock file and directory '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-zafy1i9l', purging
distributed.diskutils - INFO - Found stale lock file and directory '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-xpik_iqm', purging
distributed.diskutils - INFO - Found stale lock file and directory '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-4pii6cfr', purging
distributed.diskutils - INFO - Found stale lock file and directory '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-6os993nn', purging
distributed.diskutils - INFO - Found stale lock file and directory '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-koqz72sb', purging
distributed.diskutils - INFO - Found stale lock file and directory '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-vwxmnkuv', purging
distributed.diskutils - INFO - Found stale lock file and directory '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-tbecnqa2', purging
distributed.worker - INFO -       Start worker at:   tcp://10.148.7.215:43309
distributed.worker - INFO -          Listening to:   tcp://10.148.7.215:43309
distributed.worker - INFO -          dashboard at:         10.148.7.215:38717
distributed.worker - INFO -       Start worker at:   tcp://10.148.7.215:40287
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.17:35759
distributed.worker - INFO -          Listening to:   tcp://10.148.7.215:40287
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         10.148.7.215:39579
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.17:35759
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.148.7.215:39253
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -          Listening to:   tcp://10.148.7.215:39253
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-3hbbegq2
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -          dashboard at:         10.148.7.215:46167
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.17:35759
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-d46kovmq
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -       Start worker at:   tcp://10.148.7.215:46065
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-hqg443un
distributed.worker - INFO -          Listening to:   tcp://10.148.7.215:46065
distributed.worker - INFO -          dashboard at:         10.148.7.215:33629
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.17:35759
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-u0fyjc1k
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.148.7.215:41921
distributed.worker - INFO -          Listening to:   tcp://10.148.7.215:41921
distributed.worker - INFO -          dashboard at:         10.148.7.215:38573
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.17:35759
distributed.worker - INFO -       Start worker at:   tcp://10.148.7.215:39567
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -          Listening to:   tcp://10.148.7.215:39567
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-dxmq38qy
distributed.worker - INFO -          dashboard at:         10.148.7.215:39275
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.17:35759
distributed.worker - INFO -       Start worker at:   tcp://10.148.7.215:44565
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://10.148.7.215:44565
distributed.worker - INFO -          dashboard at:         10.148.7.215:33159
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.17:35759
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-9xx0igm6
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-z4vm3t72
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.148.7.215:36907
distributed.worker - INFO -          Listening to:   tcp://10.148.7.215:36907
distributed.worker - INFO -          dashboard at:         10.148.7.215:46491
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.17:35759
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-s3zovktg
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.148.7.215:46033
distributed.worker - INFO -          Listening to:   tcp://10.148.7.215:46033
distributed.worker - INFO -          dashboard at:         10.148.7.215:35955
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.17:35759
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-z5de5c7q
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.148.10.17:35759
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.148.10.17:35759
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.148.10.17:35759
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.148.10.17:35759
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.148.10.17:35759
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.148.10.17:35759
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.148.10.17:35759
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.148.10.17:35759
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.148.10.17:35759
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.utils_perf - INFO - full garbage collection released 40.86 MB from 3284 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 286.14 MB from 3625 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 365.81 MB from 3210 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 66.49 MB from 2358 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 94.98 MB from 3069 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 56.71 MB from 3259 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 176.82 MB from 3259 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 66.58 MB from 3369 reference cycles (threshold: 10.00 MB)
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Stopping worker at tcp://10.148.7.215:40287
distributed.worker - INFO - Stopping worker at tcp://10.148.7.215:43309
distributed.worker - INFO - Stopping worker at tcp://10.148.7.215:44565
distributed.worker - INFO - Stopping worker at tcp://10.148.7.215:46065
distributed.worker - INFO - Stopping worker at tcp://10.148.7.215:41921
distributed.worker - INFO - Stopping worker at tcp://10.148.7.215:39253
distributed.worker - INFO - Stopping worker at tcp://10.148.7.215:36907
distributed.worker - INFO - Stopping worker at tcp://10.148.7.215:39567
distributed.worker - INFO - Stopping worker at tcp://10.148.7.215:46033
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.7.215:40401'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.7.215:42819'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.7.215:41159'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.7.215:42545'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.7.215:34669'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.7.215:46481'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.7.215:33145'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.7.215:34697'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.7.215:42597'
distributed.dask_worker - INFO - End worker
