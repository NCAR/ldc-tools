distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.7.101:33131'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.7.101:46279'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.7.101:44809'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.7.101:34453'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.7.101:43837'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.7.101:38727'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.7.101:34023'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.7.101:37555'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.7.101:45727'
distributed.diskutils - INFO - Found stale lock file and directory '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-bxn97y4b', purging
distributed.diskutils - INFO - Found stale lock file and directory '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-ov0i597w', purging
distributed.diskutils - INFO - Found stale lock file and directory '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-gxrqzcid', purging
distributed.diskutils - INFO - Found stale lock file and directory '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-o6bn6u93', purging
distributed.diskutils - INFO - Found stale lock file and directory '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-b2yzasuh', purging
distributed.diskutils - INFO - Found stale lock file and directory '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-fyrxeuw8', purging
distributed.diskutils - INFO - Found stale lock file and directory '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-z0bhwag5', purging
distributed.diskutils - INFO - Found stale lock file and directory '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-0el_hjnb', purging
distributed.diskutils - INFO - Found stale lock file and directory '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-e7s2zq8r', purging
distributed.worker - INFO -       Start worker at:   tcp://10.148.7.101:41537
distributed.worker - INFO -          Listening to:   tcp://10.148.7.101:41537
distributed.worker - INFO -       Start worker at:   tcp://10.148.7.101:45353
distributed.worker - INFO -          dashboard at:         10.148.7.101:39945
distributed.worker - INFO -       Start worker at:   tcp://10.148.7.101:44993
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:45977
distributed.worker - INFO -          Listening to:   tcp://10.148.7.101:44993
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.148.7.101:44009
distributed.worker - INFO -          Listening to:   tcp://10.148.7.101:45353
distributed.worker - INFO -       Start worker at:   tcp://10.148.7.101:37219
distributed.worker - INFO -          dashboard at:         10.148.7.101:41813
distributed.worker - INFO -          dashboard at:         10.148.7.101:40769
distributed.worker - INFO -          Listening to:   tcp://10.148.7.101:37219
distributed.worker - INFO -          Listening to:   tcp://10.148.7.101:44009
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:45977
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:45977
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         10.148.7.101:42647
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         10.148.7.101:35309
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:45977
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:45977
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-8jbkh5hm
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Start worker at:   tcp://10.148.7.101:36981
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-eme99kj0
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-mv6fz4xd
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -          Listening to:   tcp://10.148.7.101:36981
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-19vekyvu
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         10.148.7.101:36249
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-_cn_l1ni
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:45977
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.148.7.101:43065
distributed.worker - INFO -          Listening to:   tcp://10.148.7.101:43065
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -          dashboard at:         10.148.7.101:40865
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-k5immqem
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:45977
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Start worker at:   tcp://10.148.7.101:37625
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-y15vl93s
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://10.148.7.101:37625
distributed.worker - INFO -          dashboard at:         10.148.7.101:37275
distributed.worker - INFO -       Start worker at:   tcp://10.148.7.101:45163
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:45977
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://10.148.7.101:45163
distributed.worker - INFO -          dashboard at:         10.148.7.101:38963
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:45977
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-k63qjhgn
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-ljznz0_k
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:45977
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:45977
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:45977
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:45977
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:45977
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:45977
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:45977
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:45977
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:45977
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 9.78 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 9.46 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 9.89 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 9.85 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 9.70 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 9.89 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 9.64 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 9.81 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 9.94 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 9.76 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 74% memory usage. Resuming worker. Process memory: 9.00 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 9.62 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 9.49 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 9.12 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 9.60 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 9.91 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 9.81 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 9.97 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 9.17 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 9.10 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 9.55 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 9.72 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 9.79 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 9.81 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 8.76 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 9.70 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 9.88 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 9.48 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 9.76 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 9.91 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 9.51 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 9.87 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 9.77 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 9.48 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 9.70 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 9.62 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 9.52 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 9.76 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 8.76 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 9.35 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 68% memory usage. Resuming worker. Process memory: 8.34 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 8.81 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 8.76 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 9.17 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 9.90 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 85% memory usage. Pausing worker.  Process memory: 10.34 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 9.69 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 9.82 GB -- Worker memory limit: 12.11 GB
distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting
distributed.worker - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 9.88 GB -- Worker memory limit: 12.11 GB
distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting
distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting
distributed.worker - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 10.11 GB -- Worker memory limit: 12.11 GB
distributed.worker - ERROR - failed during get data with tcp://10.148.7.101:37219 -> tcp://10.148.7.101:41537
Traceback (most recent call last):
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/comm/tcp.py", line 251, in write
    await future
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/worker.py", line 1248, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/comm/tcp.py", line 255, in write
    convert_stream_closed_error(self, e)
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/comm/tcp.py", line 121, in convert_stream_closed_error
    raise CommClosedError("in %s: %s: %s" % (obj, exc.__class__.__name__, exc))
distributed.comm.core.CommClosedError: in <closed TCP>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.core - INFO - Lost connection to 'tcp://10.148.7.101:35790': in <closed TCP>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.nanny - INFO - Worker process 55685 was killed by signal 15
distributed.nanny - WARNING - Restarting worker
distributed.nanny - INFO - Worker process 55686 was killed by signal 15
distributed.nanny - WARNING - Restarting worker
distributed.nanny - INFO - Worker process 55689 was killed by signal 15
distributed.nanny - WARNING - Restarting worker
distributed.worker - ERROR - Worker stream died during communication: tcp://10.148.7.101:45163
Traceback (most recent call last):
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/comm/tcp.py", line 188, in read
    n_frames = await stream.read_bytes(8)
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/worker.py", line 1953, in gather_dep
    self.rpc, deps, worker, who=self.address
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/worker.py", line 3222, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/utils_comm.py", line 391, in retry_operation
    operation=operation,
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/utils_comm.py", line 379, in retry
    return await coro()
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/worker.py", line 3209, in _get_data
    max_connections=max_connections,
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/core.py", line 541, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/comm/tcp.py", line 208, in read
    convert_stream_closed_error(self, e)
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/comm/tcp.py", line 121, in convert_stream_closed_error
    raise CommClosedError("in %s: %s: %s" % (obj, exc.__class__.__name__, exc))
distributed.comm.core.CommClosedError: in <closed TCP>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - INFO - Can't find dependencies for key ('getitem-mean_chunk-mean_agg-aggregate-741a4ac121c6c9aa7ac9cb3b23847d70', 6, 3)
distributed.worker - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 8.69 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 66% memory usage. Resuming worker. Process memory: 8.02 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 9.31 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 9.83 GB -- Worker memory limit: 12.11 GB
distributed.worker - ERROR - failed during get data with tcp://10.148.7.101:44009 -> tcp://10.148.7.101:37625
Traceback (most recent call last):
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/comm/tcp.py", line 251, in write
    await future
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/worker.py", line 1248, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/comm/tcp.py", line 255, in write
    convert_stream_closed_error(self, e)
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/comm/tcp.py", line 121, in convert_stream_closed_error
    raise CommClosedError("in %s: %s: %s" % (obj, exc.__class__.__name__, exc))
distributed.comm.core.CommClosedError: in <closed TCP>: BrokenPipeError: [Errno 32] Broken pipe
distributed.core - INFO - Lost connection to 'tcp://10.148.7.101:47456': in <closed TCP>: BrokenPipeError: [Errno 32] Broken pipe
distributed.worker - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 9.40 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 9.70 GB -- Worker memory limit: 12.11 GB
distributed.worker - INFO -       Start worker at:   tcp://10.148.7.101:46243
distributed.worker - INFO -          Listening to:   tcp://10.148.7.101:46243
distributed.worker - INFO -          dashboard at:         10.148.7.101:41075
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:45977
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-wcbz3gvc
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.148.7.101:36531
distributed.worker - INFO -          Listening to:   tcp://10.148.7.101:36531
distributed.worker - INFO -          dashboard at:         10.148.7.101:36057
distributed.worker - INFO -       Start worker at:   tcp://10.148.7.101:35815
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:45977
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://10.148.7.101:35815
distributed.worker - INFO -          dashboard at:         10.148.7.101:46189
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:45977
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-y92qi69i
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-25jmmdw7
distributed.worker - INFO - -------------------------------------------------
distributed.worker - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 9.71 GB -- Worker memory limit: 12.11 GB
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:45977
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:45977
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:45977
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 8.70 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 9.31 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 9.87 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 10.07 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 9.70 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 9.70 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 9.73 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 9.69 GB -- Worker memory limit: 12.11 GB
distributed.worker - ERROR - Worker stream died during communication: tcp://10.148.7.101:45163
Traceback (most recent call last):
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/comm/core.py", line 232, in connect
    _raise(error)
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/comm/core.py", line 213, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://10.148.7.101:45163' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2b2c9378c7d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/worker.py", line 1953, in gather_dep
    self.rpc, deps, worker, who=self.address
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/worker.py", line 3222, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/utils_comm.py", line 391, in retry_operation
    operation=operation,
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/utils_comm.py", line 379, in retry
    return await coro()
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/worker.py", line 3199, in _get_data
    comm = await rpc.connect(worker)
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/core.py", line 908, in connect
    connection_args=self.connection_args,
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/comm/core.py", line 243, in connect
    _raise(error)
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/comm/core.py", line 213, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://10.148.7.101:45163' after 10 s: Timed out trying to connect to 'tcp://10.148.7.101:45163' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2b2c9378c7d0>: ConnectionRefusedError: [Errno 111] Connection refused
distributed.worker - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 9.43 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 85% memory usage. Pausing worker.  Process memory: 10.32 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 9.43 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 9.25 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 10.27 GB -- Worker memory limit: 12.11 GB
distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting
distributed.worker - ERROR - Worker stream died during communication: tcp://10.148.7.101:44009
Traceback (most recent call last):
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/comm/tcp.py", line 188, in read
    n_frames = await stream.read_bytes(8)
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/worker.py", line 1953, in gather_dep
    self.rpc, deps, worker, who=self.address
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/worker.py", line 3222, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/utils_comm.py", line 391, in retry_operation
    operation=operation,
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/utils_comm.py", line 379, in retry
    return await coro()
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/worker.py", line 3209, in _get_data
    max_connections=max_connections,
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/core.py", line 541, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/comm/tcp.py", line 208, in read
    convert_stream_closed_error(self, e)
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/comm/tcp.py", line 121, in convert_stream_closed_error
    raise CommClosedError("in %s: %s: %s" % (obj, exc.__class__.__name__, exc))
distributed.comm.core.CommClosedError: in <closed TCP>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.nanny - INFO - Worker process 55691 was killed by signal 15
distributed.worker - INFO - Can't find dependencies for key ('rechunk-merge-703ae277aafa55203021f8461b5a0983', 1, 42, 0, 0)
distributed.worker - INFO - Dependent not found: ('rechunk-split-703ae277aafa55203021f8461b5a0983', 206) 0 .  Asking scheduler
distributed.worker - INFO - Can't find dependencies for key ('rechunk-merge-703ae277aafa55203021f8461b5a0983', 1, 46, 0, 0)
distributed.worker - INFO - Dependent not found: ('rechunk-split-703ae277aafa55203021f8461b5a0983', 215) 0 .  Asking scheduler
distributed.nanny - WARNING - Restarting worker
distributed.worker - ERROR - Worker stream died during communication: tcp://10.148.7.101:44009
Traceback (most recent call last):
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/comm/tcp.py", line 198, in read
    n = await stream.read_into(frame)
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/worker.py", line 1953, in gather_dep
    self.rpc, deps, worker, who=self.address
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/worker.py", line 3222, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/utils_comm.py", line 391, in retry_operation
    operation=operation,
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/utils_comm.py", line 379, in retry
    return await coro()
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/worker.py", line 3209, in _get_data
    max_connections=max_connections,
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/core.py", line 541, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/comm/tcp.py", line 208, in read
    convert_stream_closed_error(self, e)
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/comm/tcp.py", line 123, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc))
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed
distributed.worker - INFO - Can't find dependencies for key ('rechunk-merge-703ae277aafa55203021f8461b5a0983', 0, 25, 0, 0)
distributed.worker - INFO -       Start worker at:   tcp://10.148.7.101:41649
distributed.worker - INFO -          Listening to:   tcp://10.148.7.101:41649
distributed.worker - INFO -          dashboard at:         10.148.7.101:44115
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:45977
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-dmybl_dh
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:45977
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 9.79 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 9.57 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 9.87 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 9.61 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 9.95 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 9.32 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 9.58 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 9.40 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 9.53 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 9.83 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 9.70 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 9.42 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 10.10 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 8.62 GB -- Worker memory limit: 12.11 GB
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://10.148.7.101:36531
distributed.worker - INFO - Stopping worker at tcp://10.148.7.101:43065
distributed.worker - INFO - Stopping worker at tcp://10.148.7.101:35815
distributed.worker - INFO - Stopping worker at tcp://10.148.7.101:37219
distributed.worker - INFO - Stopping worker at tcp://10.148.7.101:46243
distributed.worker - INFO - Stopping worker at tcp://10.148.7.101:45353
distributed.worker - INFO - Stopping worker at tcp://10.148.7.101:44993
distributed.worker - INFO - Stopping worker at tcp://10.148.7.101:36981
distributed.worker - INFO - Stopping worker at tcp://10.148.7.101:41649
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.7.101:34023'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.7.101:46279'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.7.101:33131'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.7.101:37555'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.7.101:44809'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.7.101:38727'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.7.101:34453'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.7.101:43837'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.7.101:45727'
distributed.dask_worker - INFO - End worker
