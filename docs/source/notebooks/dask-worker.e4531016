distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.9.111:42533'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.9.111:36597'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.9.111:34417'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.9.111:35139'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.9.111:33665'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.9.111:43387'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.9.111:35213'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.9.111:36903'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.9.111:45975'
distributed.worker - INFO -       Start worker at:   tcp://10.148.9.111:33651
distributed.worker - INFO -          Listening to:   tcp://10.148.9.111:33651
distributed.worker - INFO -          dashboard at:         10.148.9.111:44475
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:37209
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -       Start worker at:   tcp://10.148.9.111:46785
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-ho6ahah_
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://10.148.9.111:46785
distributed.worker - INFO -          dashboard at:         10.148.9.111:35943
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:37209
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-iejui4vb
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.148.9.111:44067
distributed.worker - INFO -       Start worker at:   tcp://10.148.9.111:35141
distributed.worker - INFO -          Listening to:   tcp://10.148.9.111:44067
distributed.worker - INFO -       Start worker at:   tcp://10.148.9.111:35505
distributed.worker - INFO -          dashboard at:         10.148.9.111:36853
distributed.worker - INFO -          Listening to:   tcp://10.148.9.111:35141
distributed.worker - INFO -          Listening to:   tcp://10.148.9.111:35505
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:37209
distributed.worker - INFO -          dashboard at:         10.148.9.111:39229
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         10.148.9.111:34221
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:37209
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:37209
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-xkbyyljj
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-dxwnb531
distributed.worker - INFO -       Start worker at:   tcp://10.148.9.111:37559
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-rwyhnq80
distributed.worker - INFO -          Listening to:   tcp://10.148.9.111:37559
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         10.148.9.111:41217
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:37209
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-q440rmo2
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.148.9.111:33859
distributed.worker - INFO -          Listening to:   tcp://10.148.9.111:33859
distributed.worker - INFO -          dashboard at:         10.148.9.111:42441
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:37209
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-nyu_8qop
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:37209
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:37209
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:37209
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:37209
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:37209
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:37209
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:37209
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:   tcp://10.148.9.111:42769
distributed.worker - INFO -          Listening to:   tcp://10.148.9.111:42769
distributed.worker - INFO -          dashboard at:         10.148.9.111:44071
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:37209
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-3yy_jopf
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.148.9.111:42513
distributed.worker - INFO -          Listening to:   tcp://10.148.9.111:42513
distributed.worker - INFO -          dashboard at:         10.148.9.111:46275
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:37209
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-wog6ifvq
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:37209
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:37209
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 9.74 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 85% memory usage. Pausing worker.  Process memory: 10.31 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 9.80 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 10.08 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 9.89 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 9.96 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 9.85 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 9.64 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 9.91 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 9.61 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 9.64 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 9.46 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 9.26 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 74% memory usage. Resuming worker. Process memory: 9.06 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 10.09 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 9.51 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 8.82 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 70% memory usage. Resuming worker. Process memory: 8.51 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 9.78 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 9.39 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 10.18 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 10.16 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 10.09 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 10.10 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 9.88 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 9.74 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 9.86 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 9.49 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 9.48 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 9.19 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 9.96 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 70% memory usage. Resuming worker. Process memory: 8.54 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 9.16 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 9.40 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 10.00 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 9.69 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 10.10 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 85% memory usage. Pausing worker.  Process memory: 10.35 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 9.92 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 8.65 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 74% memory usage. Resuming worker. Process memory: 8.99 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 9.23 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 86% memory usage. Pausing worker.  Process memory: 10.46 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 8.75 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 9.30 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 9.12 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 9.36 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 9.58 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 86% memory usage. Pausing worker.  Process memory: 10.50 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 85% memory usage. Pausing worker.  Process memory: 10.30 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 9.69 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 10.28 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 9.89 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 8.76 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 9.39 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 68% memory usage. Resuming worker. Process memory: 8.31 GB -- Worker memory limit: 12.11 GB
distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting
distributed.nanny - INFO - Worker process 10817 was killed by signal 15
distributed.worker - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 9.48 GB -- Worker memory limit: 12.11 GB
distributed.worker - ERROR - failed during get data with tcp://10.148.9.111:37559 -> tcp://10.148.9.111:35505
Traceback (most recent call last):
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/comm/tcp.py", line 188, in read
    n_frames = await stream.read_bytes(8)
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/worker.py", line 1249, in get_data
    response = await comm.read(deserializers=serializers)
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/comm/tcp.py", line 208, in read
    convert_stream_closed_error(self, e)
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/comm/tcp.py", line 121, in convert_stream_closed_error
    raise CommClosedError("in %s: %s: %s" % (obj, exc.__class__.__name__, exc))
distributed.comm.core.CommClosedError: in <closed TCP>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.core - INFO - Lost connection to 'tcp://10.148.9.111:47172': in <closed TCP>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.nanny - WARNING - Restarting worker
distributed.worker - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 10.07 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 9.50 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 10.13 GB -- Worker memory limit: 12.11 GB
distributed.worker - INFO -       Start worker at:   tcp://10.148.9.111:46145
distributed.worker - INFO -          Listening to:   tcp://10.148.9.111:46145
distributed.worker - INFO -          dashboard at:         10.148.9.111:44675
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:37209
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-5nv7hnyx
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:37209
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 9.65 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 88% memory usage. Pausing worker.  Process memory: 10.67 GB -- Worker memory limit: 12.11 GB
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2ba34c10e4d0>
Traceback (most recent call last):
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/tornado/ioloop.py", line 907, in _run
    return self.callback()
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/worker.py", line 652, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}),
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/batched.py", line 117, in send
    raise CommClosedError
distributed.comm.core.CommClosedError
distributed.worker - INFO - Stopping worker at tcp://10.148.9.111:33859
distributed.worker - INFO - Stopping worker at tcp://10.148.9.111:46785
distributed.worker - INFO - Stopping worker at tcp://10.148.9.111:35141
distributed.worker - INFO - Stopping worker at tcp://10.148.9.111:42769
distributed.worker - INFO - Stopping worker at tcp://10.148.9.111:37559
distributed.worker - INFO - Stopping worker at tcp://10.148.9.111:44067
distributed.worker - INFO - Stopping worker at tcp://10.148.9.111:46145
distributed.worker - INFO - Stopping worker at tcp://10.148.9.111:42513
distributed.worker - INFO - Stopping worker at tcp://10.148.9.111:33651
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.9.111:33665'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.9.111:36903'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.9.111:42533'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.9.111:35139'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.9.111:43387'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.9.111:45975'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.9.111:34417'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.9.111:35213'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.9.111:36597'
distributed.dask_worker - INFO - End worker
