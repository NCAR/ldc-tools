distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.11.40:46003'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.11.40:40689'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.11.40:42881'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.11.40:37575'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.11.40:39767'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.11.40:40319'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.11.40:46021'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.11.40:39089'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.11.40:38671'
distributed.worker - INFO -       Start worker at:   tcp://10.148.11.40:42611
distributed.worker - INFO -          Listening to:   tcp://10.148.11.40:42611
distributed.worker - INFO -       Start worker at:   tcp://10.148.11.40:34621
distributed.worker - INFO -          dashboard at:         10.148.11.40:37839
distributed.worker - INFO -          Listening to:   tcp://10.148.11.40:34621
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:36241
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         10.148.11.40:35175
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:36241
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-6298t6ox
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-pcam85qc
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.148.11.40:37065
distributed.worker - INFO -          Listening to:   tcp://10.148.11.40:37065
distributed.worker - INFO -          dashboard at:         10.148.11.40:39025
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:36241
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Start worker at:   tcp://10.148.11.40:43601
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-wtsa47v3
distributed.worker - INFO -          Listening to:   tcp://10.148.11.40:43601
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         10.148.11.40:40937
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:36241
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-0321c121
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.148.11.40:43349
distributed.worker - INFO -          Listening to:   tcp://10.148.11.40:43349
distributed.worker - INFO -          dashboard at:         10.148.11.40:34365
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:36241
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-dyyya73f
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.148.11.40:37241
distributed.worker - INFO -       Start worker at:   tcp://10.148.11.40:41271
distributed.worker - INFO -          Listening to:   tcp://10.148.11.40:41271
distributed.worker - INFO -          Listening to:   tcp://10.148.11.40:37241
distributed.worker - INFO -          dashboard at:         10.148.11.40:40383
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:36241
distributed.worker - INFO -          dashboard at:         10.148.11.40:40645
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:36241
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-j7olx52u
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-nmrooggw
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:36241
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:   tcp://10.148.11.40:36917
distributed.worker - INFO -          Listening to:   tcp://10.148.11.40:36917
distributed.worker - INFO -          dashboard at:         10.148.11.40:34699
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:36241
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-fwfn6cgs
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:36241
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:36241
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:36241
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:36241
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:36241
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:   tcp://10.148.11.40:43723
distributed.worker - INFO -          Listening to:   tcp://10.148.11.40:43723
distributed.worker - INFO -          dashboard at:         10.148.11.40:43347
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:36241
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-vced3odk
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:36241
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:36241
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:36241
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 9.89 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 9.67 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 9.72 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 74% memory usage. Resuming worker. Process memory: 8.98 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 9.74 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 73% memory usage. Resuming worker. Process memory: 8.93 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 9.79 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 9.40 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 9.97 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 9.81 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 9.29 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 8.64 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 9.79 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 74% memory usage. Resuming worker. Process memory: 9.01 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 9.70 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 10.19 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 9.94 GB -- Worker memory limit: 12.11 GB
distributed.utils_perf - INFO - full garbage collection released 212.83 MB from 929 reference cycles (threshold: 10.00 MB)
distributed.worker - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 9.88 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 9.70 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 10.09 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 9.64 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 10.17 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 9.91 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 9.57 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 9.97 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 10.20 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 9.51 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 10.27 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 9.81 GB -- Worker memory limit: 12.11 GB
distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting
distributed.worker - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 9.66 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 10.17 GB -- Worker memory limit: 12.11 GB
distributed.nanny - INFO - Worker process 71309 was killed by signal 15
distributed.nanny - WARNING - Restarting worker
distributed.worker - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 9.49 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 8.81 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 9.31 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 74% memory usage. Resuming worker. Process memory: 9.02 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 9.47 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 9.21 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 10.20 GB -- Worker memory limit: 12.11 GB
distributed.diskutils - INFO - Found stale lock file and directory '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-jy3pidsv', purging
distributed.diskutils - INFO - Found stale lock file and directory '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-0gicxsof', purging
distributed.diskutils - INFO - Found stale lock file and directory '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-gi_80qam', purging
distributed.diskutils - INFO - Found stale lock file and directory '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-1jt09ect', purging
distributed.diskutils - INFO - Found stale lock file and directory '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-ivgb7zdu', purging
distributed.diskutils - INFO - Found stale lock file and directory '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-5a3y3uob', purging
distributed.diskutils - INFO - Found stale lock file and directory '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-l9z18idi', purging
distributed.diskutils - INFO - Found stale lock file and directory '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-iii1aipt', purging
distributed.diskutils - INFO - Found stale lock file and directory '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-yovzpva1', purging
distributed.worker - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 10.13 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 9.48 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 9.97 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 10.06 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 74% memory usage. Resuming worker. Process memory: 8.99 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 9.86 GB -- Worker memory limit: 12.11 GB
distributed.worker - INFO -       Start worker at:   tcp://10.148.11.40:38483
distributed.worker - INFO -          Listening to:   tcp://10.148.11.40:38483
distributed.worker - INFO -          dashboard at:         10.148.11.40:35461
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:36241
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-cq7tox6k
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:36241
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting
distributed.worker - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 9.85 GB -- Worker memory limit: 12.11 GB
distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting
distributed.worker - ERROR - failed during get data with tcp://10.148.11.40:37065 -> tcp://10.148.11.40:42611
Traceback (most recent call last):
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/comm/tcp.py", line 188, in read
    n_frames = await stream.read_bytes(8)
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/worker.py", line 1249, in get_data
    response = await comm.read(deserializers=serializers)
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/comm/tcp.py", line 208, in read
    convert_stream_closed_error(self, e)
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/comm/tcp.py", line 123, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc))
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed
distributed.core - INFO - Lost connection to 'tcp://10.148.11.40:33746': in <closed TCP>: Stream is closed
distributed.nanny - INFO - Worker process 71304 was killed by signal 15
distributed.nanny - WARNING - Restarting worker
distributed.nanny - INFO - Worker process 71292 was killed by signal 15
distributed.nanny - WARNING - Restarting worker
distributed.worker - INFO -       Start worker at:   tcp://10.148.11.40:41541
distributed.worker - INFO -          Listening to:   tcp://10.148.11.40:41541
distributed.worker - INFO -          dashboard at:         10.148.11.40:40667
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:36241
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-kayyl2iy
distributed.worker - INFO - -------------------------------------------------
distributed.worker - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 9.69 GB -- Worker memory limit: 12.11 GB
distributed.worker - INFO -       Start worker at:   tcp://10.148.11.40:44281
distributed.worker - INFO -          Listening to:   tcp://10.148.11.40:44281
distributed.worker - INFO -          dashboard at:         10.148.11.40:38315
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:36241
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-6f3u7vm3
distributed.worker - INFO - -------------------------------------------------
distributed.worker - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 9.75 GB -- Worker memory limit: 12.11 GB
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:36241
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:36241
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - WARNING - Worker is at 73% memory usage. Resuming worker. Process memory: 8.96 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 10.03 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 9.66 GB -- Worker memory limit: 12.11 GB
distributed.worker - INFO - Failed to put key in memory
Traceback (most recent call last):
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/worker.py", line 1671, in transition_executing_done
    self.put_key_in_memory(key, value, transition=False)
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/worker.py", line 1894, in put_key_in_memory
    self.data[key] = value
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/zict/buffer.py", line 87, in __setitem__
    self.fast[key] = value
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/zict/lru.py", line 70, in __setitem__
    self.evict()
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/zict/lru.py", line 89, in evict
    cb(k, v)
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/zict/buffer.py", line 60, in fast_to_slow
    self.slow[key] = value
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/zict/func.py", line 41, in __setitem__
    self.d[key] = self.dump(value)
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/zict/file.py", line 79, in __setitem__
    with open(os.path.join(self.directory, _safe_key(key)), "wb") as f:
FileNotFoundError: [Errno 2] No such file or directory: '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-vced3odk/storage/%28%27concatenate-a0041d107fd9d61517d05c8ec35457f1%27%2C%201%2C%2016%2C%200%2C%200%29'
distributed.worker - INFO - Failed to put key in memory
Traceback (most recent call last):
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/worker.py", line 1671, in transition_executing_done
    self.put_key_in_memory(key, value, transition=False)
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/worker.py", line 1894, in put_key_in_memory
    self.data[key] = value
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/zict/buffer.py", line 87, in __setitem__
    self.fast[key] = value
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/zict/lru.py", line 70, in __setitem__
    self.evict()
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/zict/lru.py", line 89, in evict
    cb(k, v)
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/zict/buffer.py", line 60, in fast_to_slow
    self.slow[key] = value
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/zict/func.py", line 41, in __setitem__
    self.d[key] = self.dump(value)
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/zict/file.py", line 79, in __setitem__
    with open(os.path.join(self.directory, _safe_key(key)), "wb") as f:
FileNotFoundError: [Errno 2] No such file or directory: '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-vced3odk/storage/%28%27concatenate-a0041d107fd9d61517d05c8ec35457f1%27%2C%201%2C%2025%2C%200%2C%200%29'
distributed.worker - INFO - Failed to put key in memory
Traceback (most recent call last):
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/worker.py", line 1671, in transition_executing_done
    self.put_key_in_memory(key, value, transition=False)
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/worker.py", line 1894, in put_key_in_memory
    self.data[key] = value
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/zict/buffer.py", line 87, in __setitem__
    self.fast[key] = value
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/zict/lru.py", line 70, in __setitem__
    self.evict()
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/zict/lru.py", line 89, in evict
    cb(k, v)
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/zict/buffer.py", line 60, in fast_to_slow
    self.slow[key] = value
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/zict/func.py", line 41, in __setitem__
    self.d[key] = self.dump(value)
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/zict/file.py", line 79, in __setitem__
    with open(os.path.join(self.directory, _safe_key(key)), "wb") as f:
FileNotFoundError: [Errno 2] No such file or directory: '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-vced3odk/storage/%28%27concatenate-a0041d107fd9d61517d05c8ec35457f1%27%2C%201%2C%2038%2C%200%2C%200%29'
distributed.worker - INFO - Failed to put key in memory
Traceback (most recent call last):
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/worker.py", line 1671, in transition_executing_done
    self.put_key_in_memory(key, value, transition=False)
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/worker.py", line 1894, in put_key_in_memory
    self.data[key] = value
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/zict/buffer.py", line 87, in __setitem__
    self.fast[key] = value
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/zict/lru.py", line 70, in __setitem__
    self.evict()
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/zict/lru.py", line 89, in evict
    cb(k, v)
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/zict/buffer.py", line 60, in fast_to_slow
    self.slow[key] = value
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/zict/func.py", line 41, in __setitem__
    self.d[key] = self.dump(value)
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/zict/file.py", line 79, in __setitem__
    with open(os.path.join(self.directory, _safe_key(key)), "wb") as f:
FileNotFoundError: [Errno 2] No such file or directory: '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-vced3odk/storage/%28%27concatenate-a0041d107fd9d61517d05c8ec35457f1%27%2C%201%2C%2043%2C%200%2C%200%29'
distributed.worker - INFO - Failed to put key in memory
Traceback (most recent call last):
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/worker.py", line 1671, in transition_executing_done
    self.put_key_in_memory(key, value, transition=False)
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/worker.py", line 1894, in put_key_in_memory
    self.data[key] = value
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/zict/buffer.py", line 87, in __setitem__
    self.fast[key] = value
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/zict/lru.py", line 70, in __setitem__
    self.evict()
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/zict/lru.py", line 89, in evict
    cb(k, v)
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/zict/buffer.py", line 60, in fast_to_slow
    self.slow[key] = value
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/zict/func.py", line 41, in __setitem__
    self.d[key] = self.dump(value)
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/zict/file.py", line 79, in __setitem__
    with open(os.path.join(self.directory, _safe_key(key)), "wb") as f:
FileNotFoundError: [Errno 2] No such file or directory: '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-vced3odk/storage/%28%27concatenate-a0041d107fd9d61517d05c8ec35457f1%27%2C%201%2C%2052%2C%200%2C%200%29'
distributed.utils_perf - INFO - full garbage collection released 451.83 MB from 747 reference cycles (threshold: 10.00 MB)
distributed.worker - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 9.87 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 9.41 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 9.76 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 9.75 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 9.63 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 9.82 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 9.41 GB -- Worker memory limit: 12.11 GB
distributed.utils_perf - INFO - full garbage collection released 180.71 MB from 333 reference cycles (threshold: 10.00 MB)
distributed.worker - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 9.91 GB -- Worker memory limit: 12.11 GB
distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting
distributed.worker - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 9.38 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 9.97 GB -- Worker memory limit: 12.11 GB
distributed.worker - ERROR - Worker stream died during communication: tcp://10.148.11.40:43349
Traceback (most recent call last):
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/comm/tcp.py", line 198, in read
    n = await stream.read_into(frame)
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/worker.py", line 1953, in gather_dep
    self.rpc, deps, worker, who=self.address
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/worker.py", line 3222, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/utils_comm.py", line 391, in retry_operation
    operation=operation,
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/utils_comm.py", line 379, in retry
    return await coro()
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/worker.py", line 3209, in _get_data
    max_connections=max_connections,
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/core.py", line 541, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/comm/tcp.py", line 208, in read
    convert_stream_closed_error(self, e)
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/comm/tcp.py", line 123, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc))
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed
distributed.worker - ERROR - Worker stream died during communication: tcp://10.148.11.40:43349
Traceback (most recent call last):
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/comm/tcp.py", line 188, in read
    n_frames = await stream.read_bytes(8)
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/worker.py", line 1953, in gather_dep
    self.rpc, deps, worker, who=self.address
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/worker.py", line 3222, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/utils_comm.py", line 391, in retry_operation
    operation=operation,
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/utils_comm.py", line 379, in retry
    return await coro()
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/worker.py", line 3209, in _get_data
    max_connections=max_connections,
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/core.py", line 541, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/comm/tcp.py", line 208, in read
    convert_stream_closed_error(self, e)
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/comm/tcp.py", line 121, in convert_stream_closed_error
    raise CommClosedError("in %s: %s: %s" % (obj, exc.__class__.__name__, exc))
distributed.comm.core.CommClosedError: in <closed TCP>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.nanny - INFO - Worker process 71296 was killed by signal 15
distributed.worker - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 9.75 GB -- Worker memory limit: 12.11 GB
distributed.nanny - WARNING - Restarting worker
distributed.worker - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 9.44 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 9.48 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 9.48 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 9.82 GB -- Worker memory limit: 12.11 GB
distributed.worker - INFO -       Start worker at:   tcp://10.148.11.40:35511
distributed.worker - INFO -          Listening to:   tcp://10.148.11.40:35511
distributed.worker - INFO -          dashboard at:         10.148.11.40:42259
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:36241
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-inuwmmpi
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:36241
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 9.88 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 74% memory usage. Resuming worker. Process memory: 9.01 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 9.61 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 9.48 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 9.71 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 85% memory usage. Pausing worker.  Process memory: 10.31 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 9.72 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 9.58 GB -- Worker memory limit: 12.11 GB
distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting
distributed.worker - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 9.82 GB -- Worker memory limit: 12.11 GB
distributed.worker - ERROR - Worker stream died during communication: tcp://10.148.11.40:38483
Traceback (most recent call last):
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/comm/tcp.py", line 188, in read
    n_frames = await stream.read_bytes(8)
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/worker.py", line 1953, in gather_dep
    self.rpc, deps, worker, who=self.address
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/worker.py", line 3222, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/utils_comm.py", line 391, in retry_operation
    operation=operation,
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/utils_comm.py", line 379, in retry
    return await coro()
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/worker.py", line 3209, in _get_data
    max_connections=max_connections,
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/core.py", line 541, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/comm/tcp.py", line 208, in read
    convert_stream_closed_error(self, e)
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/comm/tcp.py", line 121, in convert_stream_closed_error
    raise CommClosedError("in %s: %s: %s" % (obj, exc.__class__.__name__, exc))
distributed.comm.core.CommClosedError: in <closed TCP>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - INFO - Can't find dependencies for key ('rechunk-merge-703ae277aafa55203021f8461b5a0983', 0, 19, 0, 0)
distributed.nanny - INFO - Worker process 72348 was killed by signal 15
distributed.worker - INFO - Dependent not found: ('rechunk-split-703ae277aafa55203021f8461b5a0983', 43) 0 .  Asking scheduler
distributed.worker - INFO - Dependent not found: ('concatenate-9306761062214e1a899c27fe99e647bd', 0, 23, 0, 0) 0 .  Asking scheduler
distributed.nanny - WARNING - Restarting worker
distributed.worker - INFO -       Start worker at:   tcp://10.148.11.40:41373
distributed.worker - INFO -          Listening to:   tcp://10.148.11.40:41373
distributed.worker - INFO -          dashboard at:         10.148.11.40:36597
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:36241
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-u1xienul
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:36241
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 9.78 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 9.63 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 9.71 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 9.70 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 9.23 GB -- Worker memory limit: 12.11 GB
distributed.worker - ERROR - Worker stream died during communication: tcp://10.148.11.40:43349
Traceback (most recent call last):
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/comm/core.py", line 232, in connect
    _raise(error)
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/comm/core.py", line 213, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://10.148.11.40:43349' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2af4c3e10710>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/worker.py", line 1953, in gather_dep
    self.rpc, deps, worker, who=self.address
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/worker.py", line 3222, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/utils_comm.py", line 391, in retry_operation
    operation=operation,
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/utils_comm.py", line 379, in retry
    return await coro()
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/worker.py", line 3199, in _get_data
    comm = await rpc.connect(worker)
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/core.py", line 908, in connect
    connection_args=self.connection_args,
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/comm/core.py", line 243, in connect
    _raise(error)
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/comm/core.py", line 213, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://10.148.11.40:43349' after 10 s: Timed out trying to connect to 'tcp://10.148.11.40:43349' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2af4c3e10710>: ConnectionRefusedError: [Errno 111] Connection refused
distributed.worker - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 9.61 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 9.65 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 9.72 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 9.50 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 9.86 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 10.14 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 9.83 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 9.61 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 88% memory usage. Pausing worker.  Process memory: 10.66 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 9.52 GB -- Worker memory limit: 12.11 GB
distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting
distributed.nanny - INFO - Worker process 71297 was killed by signal 15
distributed.nanny - WARNING - Restarting worker
distributed.worker - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 9.71 GB -- Worker memory limit: 12.11 GB
distributed.worker - INFO -       Start worker at:   tcp://10.148.11.40:42967
distributed.worker - INFO -          Listening to:   tcp://10.148.11.40:42967
distributed.worker - INFO -          dashboard at:         10.148.11.40:35021
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:36241
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-c6jqi_ve
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:36241
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 9.31 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 55% memory usage. Resuming worker. Process memory: 6.76 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 66% memory usage. Resuming worker. Process memory: 8.06 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 9.37 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 53% memory usage. Resuming worker. Process memory: 6.47 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 50% memory usage. Resuming worker. Process memory: 6.09 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 9.74 GB -- Worker memory limit: 12.11 GB
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2b12d0d974d0>
Traceback (most recent call last):
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/tornado/ioloop.py", line 907, in _run
    return self.callback()
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/worker.py", line 652, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}),
  File "/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/site-packages/distributed/batched.py", line 117, in send
    raise CommClosedError
distributed.comm.core.CommClosedError
distributed.worker - INFO - Stopping worker at tcp://10.148.11.40:42967
distributed.worker - INFO - Stopping worker at tcp://10.148.11.40:44281
distributed.worker - INFO - Stopping worker at tcp://10.148.11.40:43601
distributed.worker - INFO - Stopping worker at tcp://10.148.11.40:41541
distributed.worker - INFO - Stopping worker at tcp://10.148.11.40:35511
distributed.diskutils - ERROR - Failed to remove '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-0321c121' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-0321c121'
distributed.worker - INFO - Stopping worker at tcp://10.148.11.40:36917
distributed.worker - INFO - Stopping worker at tcp://10.148.11.40:43723
distributed.worker - INFO - Stopping worker at tcp://10.148.11.40:41373
distributed.worker - INFO - Stopping worker at tcp://10.148.11.40:37065
distributed.diskutils - ERROR - Failed to remove '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-fwfn6cgs' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-fwfn6cgs'
distributed.diskutils - ERROR - Failed to remove '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-vced3odk' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-vced3odk'
distributed.diskutils - ERROR - Failed to remove '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-wtsa47v3' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-wtsa47v3'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.11.40:39767'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.11.40:38671'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.11.40:42881'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.11.40:39089'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.11.40:40689'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.11.40:46021'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.11.40:40319'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.11.40:37575'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.11.40:46003'
distributed.dask_worker - INFO - End worker
