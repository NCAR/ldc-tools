distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.9.237:34775'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.9.237:46455'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.9.237:35059'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.9.237:34725'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.9.237:34951'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.9.237:46007'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.9.237:39583'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.9.237:41121'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.9.237:45863'
distributed.diskutils - INFO - Found stale lock file and directory '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-akynmi4n', purging
/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://10.148.9.237:42451
distributed.worker - INFO -          Listening to:   tcp://10.148.9.237:42451
distributed.worker - INFO -          dashboard at:         10.148.9.237:44819
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:39071
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-l1zfqr9z
distributed.worker - INFO - -------------------------------------------------
/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://10.148.9.237:36331
/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://10.148.9.237:42831
distributed.worker - INFO -          Listening to:   tcp://10.148.9.237:36331
distributed.worker - INFO -          Listening to:   tcp://10.148.9.237:42831
distributed.worker - INFO -          dashboard at:         10.148.9.237:37029
distributed.worker - INFO -          dashboard at:         10.148.9.237:42653
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:39071
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:39071
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-876fbkts
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-6pxn3ndn
distributed.worker - INFO - -------------------------------------------------
/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://10.148.9.237:33545
distributed.worker - INFO -          Listening to:   tcp://10.148.9.237:33545
distributed.worker - INFO -          dashboard at:         10.148.9.237:43685
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:39071
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-_x4e1iqk
distributed.worker - INFO - -------------------------------------------------
/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://10.148.9.237:43979
distributed.worker - INFO -          Listening to:   tcp://10.148.9.237:43979
distributed.worker - INFO -          dashboard at:         10.148.9.237:46661
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:39071
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-tami8gtf
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:39071
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://10.148.9.237:46407
distributed.worker - INFO -          Listening to:   tcp://10.148.9.237:46407
distributed.worker - INFO -          dashboard at:         10.148.9.237:41591
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:39071
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-8744q0wk
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:39071
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://10.148.9.237:39567
distributed.worker - INFO -          Listening to:   tcp://10.148.9.237:39567
distributed.worker - INFO -          dashboard at:         10.148.9.237:42531
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:39071
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-7_i3ij5l
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:39071
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:39071
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://10.148.9.237:35777
distributed.worker - INFO -          Listening to:   tcp://10.148.9.237:35777
distributed.worker - INFO -          dashboard at:         10.148.9.237:44331
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:39071
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-jqwz8tos
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:39071
distributed.worker - INFO - -------------------------------------------------
/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://10.148.9.237:33691
distributed.core - INFO - Starting established connection
distributed.worker - INFO -          Listening to:   tcp://10.148.9.237:33691
distributed.worker - INFO -          dashboard at:         10.148.9.237:44703
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:39071
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-s2pf8o2e
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:39071
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:39071
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:39071
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:39071
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Event loop was unresponsive in Worker for 3.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 9.90 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 10.10 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 49% memory usage. Resuming worker. Process memory: 6.05 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 42% memory usage. Resuming worker. Process memory: 5.13 GB -- Worker memory limit: 12.11 GB
distributed.utils_perf - INFO - full garbage collection released 303.44 MB from 330 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 198.83 MB from 606 reference cycles (threshold: 10.00 MB)
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://10.148.9.237:33545
distributed.worker - INFO - Stopping worker at tcp://10.148.9.237:43979
distributed.worker - INFO - Stopping worker at tcp://10.148.9.237:36331
distributed.worker - INFO - Stopping worker at tcp://10.148.9.237:46407
distributed.worker - INFO - Stopping worker at tcp://10.148.9.237:33691
distributed.worker - INFO - Stopping worker at tcp://10.148.9.237:42451
distributed.worker - INFO - Stopping worker at tcp://10.148.9.237:35777
distributed.worker - INFO - Stopping worker at tcp://10.148.9.237:42831
distributed.worker - INFO - Stopping worker at tcp://10.148.9.237:39567
distributed.diskutils - ERROR - Failed to remove '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-6pxn3ndn' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-6pxn3ndn'
distributed.diskutils - ERROR - Failed to remove '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-l1zfqr9z' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-l1zfqr9z'
distributed.diskutils - ERROR - Failed to remove '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-tami8gtf' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-tami8gtf'
distributed.diskutils - ERROR - Failed to remove '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-8744q0wk' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-8744q0wk'
distributed.diskutils - ERROR - Failed to remove '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-7_i3ij5l' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-7_i3ij5l'
distributed.diskutils - ERROR - Failed to remove '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-jqwz8tos' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-jqwz8tos'
distributed.diskutils - ERROR - Failed to remove '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-876fbkts' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-876fbkts'
distributed.diskutils - ERROR - Failed to remove '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-_x4e1iqk' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-_x4e1iqk'
distributed.diskutils - ERROR - Failed to remove '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-s2pf8o2e' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-s2pf8o2e'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.9.237:39583'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.9.237:41121'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.9.237:46007'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.9.237:45863'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.9.237:34775'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.9.237:35059'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.9.237:46455'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.9.237:34951'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.9.237:34725'
distributed.dask_worker - INFO - End worker
