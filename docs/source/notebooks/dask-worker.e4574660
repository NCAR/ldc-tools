distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.7.142:33583'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.7.142:36359'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.7.142:43087'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.7.142:45895'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.7.142:39567'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.7.142:46273'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.7.142:43717'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.7.142:40053'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.7.142:44355'
distributed.diskutils - INFO - Found stale lock file and directory '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-pnm8urun', purging
distributed.diskutils - INFO - Found stale lock file and directory '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-mdpl9klj', purging
distributed.diskutils - INFO - Found stale lock file and directory '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-vq1vsyze', purging
distributed.diskutils - INFO - Found stale lock file and directory '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-9w5c2nlq', purging
distributed.diskutils - INFO - Found stale lock file and directory '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-4_0_xqsi', purging
distributed.diskutils - INFO - Found stale lock file and directory '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-j3ypopg0', purging
distributed.diskutils - ERROR - Failed to remove '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-j3ypopg0/storage' (failed in <built-in function rmdir>): [Errno 2] No such file or directory: 'storage'
distributed.diskutils - INFO - Found stale lock file and directory '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-62qoyqst', purging
distributed.worker - INFO -       Start worker at:   tcp://10.148.7.142:40783
/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://10.148.7.142:44923
distributed.worker - INFO -       Start worker at:   tcp://10.148.7.142:40667
/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://10.148.7.142:35809
/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://10.148.7.142:40549
distributed.worker - INFO -          Listening to:   tcp://10.148.7.142:40783
distributed.worker - INFO -          Listening to:   tcp://10.148.7.142:40667
distributed.worker - INFO -          Listening to:   tcp://10.148.7.142:44923
/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://10.148.7.142:38507
distributed.worker - INFO -          Listening to:   tcp://10.148.7.142:40549
distributed.worker - INFO -          dashboard at:         10.148.7.142:34391
distributed.worker - INFO -          dashboard at:         10.148.7.142:35477
distributed.worker - INFO -          Listening to:   tcp://10.148.7.142:38507
distributed.worker - INFO -          dashboard at:         10.148.7.142:36929
distributed.worker - INFO - Waiting to connect to:  tcp://10.148.13.152:37501
distributed.worker - INFO - Waiting to connect to:  tcp://10.148.13.152:37501
distributed.worker - INFO -          Listening to:   tcp://10.148.7.142:35809
distributed.worker - INFO - Waiting to connect to:  tcp://10.148.13.152:37501
distributed.worker - INFO -          dashboard at:         10.148.7.142:38303
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:  tcp://10.148.13.152:37501
distributed.worker - INFO -          dashboard at:         10.148.7.142:39651
distributed.worker - INFO -          dashboard at:         10.148.7.142:39713
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:  tcp://10.148.13.152:37501
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO - Waiting to connect to:  tcp://10.148.13.152:37501
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-6o86nok6
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-nj6ni5ld
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-oabjxyuy
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-yp19wefx
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-21j0963h
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-49z9jb05
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://10.148.7.142:38643
distributed.worker - INFO -          Listening to:   tcp://10.148.7.142:38643
distributed.worker - INFO -          dashboard at:         10.148.7.142:41681
distributed.worker - INFO - Waiting to connect to:  tcp://10.148.13.152:37501
distributed.worker - INFO - -------------------------------------------------
/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://10.148.7.142:41383
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -          Listening to:   tcp://10.148.7.142:41383
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -          dashboard at:         10.148.7.142:35499
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-nnh4_vow
distributed.worker - INFO - Waiting to connect to:  tcp://10.148.13.152:37501
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-u4obrafh
distributed.worker - INFO - -------------------------------------------------
/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://10.148.7.142:36713
distributed.worker - INFO -          Listening to:   tcp://10.148.7.142:36713
distributed.worker - INFO -          dashboard at:         10.148.7.142:46629
distributed.worker - INFO - Waiting to connect to:  tcp://10.148.13.152:37501
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-qrcpdo3u
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:  tcp://10.148.13.152:37501
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:  tcp://10.148.13.152:37501
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:  tcp://10.148.13.152:37501
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:  tcp://10.148.13.152:37501
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:  tcp://10.148.13.152:37501
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:  tcp://10.148.13.152:37501
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:  tcp://10.148.13.152:37501
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:  tcp://10.148.13.152:37501
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:  tcp://10.148.13.152:37501
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Stopping worker at tcp://10.148.7.142:40667
distributed.worker - INFO - Stopping worker at tcp://10.148.7.142:36713
distributed.worker - INFO - Stopping worker at tcp://10.148.7.142:44923
distributed.worker - INFO - Stopping worker at tcp://10.148.7.142:35809
distributed.worker - INFO - Stopping worker at tcp://10.148.7.142:40783
distributed.worker - INFO - Stopping worker at tcp://10.148.7.142:40549
distributed.diskutils - ERROR - Failed to remove '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-49z9jb05' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-49z9jb05'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.7.142:33583'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.7.142:36359'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.7.142:43087'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.7.142:45895'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.7.142:39567'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.7.142:46273'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.7.142:43717'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.7.142:40053'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.7.142:44355'
distributed.nanny - INFO - Worker process 36365 was killed by signal 15
distributed.nanny - INFO - Worker process 36359 was killed by signal 15
distributed.nanny - INFO - Worker process 36347 was killed by signal 15
distributed.dask_worker - INFO - End worker
