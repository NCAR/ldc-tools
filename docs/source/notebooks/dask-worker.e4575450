distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.10.5:38053'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.10.5:41039'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.10.5:33821'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.10.5:45853'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.10.5:45769'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.10.5:39453'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.10.5:41169'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.10.5:44119'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.10.5:42747'
distributed.diskutils - INFO - Found stale lock file and directory '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-gajtrv9c', purging
distributed.diskutils - ERROR - Failed to remove '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-gajtrv9c' (failed in <built-in function rmdir>): [Errno 2] No such file or directory: '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-gajtrv9c'
distributed.diskutils - INFO - Found stale lock file and directory '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-kqg_3pia', purging
distributed.diskutils - INFO - Found stale lock file and directory '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-3m8asc99', purging
distributed.diskutils - INFO - Found stale lock file and directory '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-yu4snlu2', purging
distributed.diskutils - ERROR - Failed to remove '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-yu4snlu2/storage' (failed in <built-in function unlink>): [Errno 20] Not a directory: 'storage'
distributed.diskutils - ERROR - Failed to remove '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-yu4snlu2' (failed in <built-in function rmdir>): [Errno 2] No such file or directory: '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-yu4snlu2'
distributed.diskutils - INFO - Found stale lock file and directory '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-6c15qh86', purging
distributed.diskutils - ERROR - Failed to remove '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-6c15qh86/storage' (failed in <built-in function open>): [Errno 2] No such file or directory: 'storage'
distributed.diskutils - ERROR - Failed to remove '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-6c15qh86' (failed in <built-in function rmdir>): [Errno 2] No such file or directory: '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-6c15qh86'
distributed.diskutils - INFO - Found stale lock file and directory '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-iciwsr93', purging
distributed.diskutils - ERROR - Failed to remove '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-iciwsr93' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-iciwsr93'
distributed.diskutils - INFO - Found stale lock file and directory '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-w3rhh3i8', purging
distributed.diskutils - ERROR - Failed to remove '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-w3rhh3i8' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-w3rhh3i8'
/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://10.148.10.5:43617
/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://10.148.10.5:35973
distributed.worker - INFO -          Listening to:    tcp://10.148.10.5:43617
distributed.worker - INFO -          Listening to:    tcp://10.148.10.5:35973
distributed.worker - INFO -          dashboard at:          10.148.10.5:44303
distributed.worker - INFO -          dashboard at:          10.148.10.5:34935
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:39939
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:39939
/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://10.148.10.5:43109
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -          Listening to:    tcp://10.148.10.5:43109
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -          dashboard at:          10.148.10.5:46625
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-prmzpd1k
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-bu8fm114
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:39939
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -                Memory:                   12.11 GB
/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://10.148.10.5:45357
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-dfi9yjzc
distributed.worker - INFO - -------------------------------------------------
/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://10.148.10.5:43885
distributed.worker - INFO -          Listening to:    tcp://10.148.10.5:43885
/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://10.148.10.5:43343
distributed.worker - INFO -          Listening to:    tcp://10.148.10.5:45357
distributed.worker - INFO -          dashboard at:          10.148.10.5:38089
distributed.worker - INFO -          Listening to:    tcp://10.148.10.5:43343
/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://10.148.10.5:32941
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:39939
distributed.worker - INFO -          dashboard at:          10.148.10.5:37303
distributed.worker - INFO -          dashboard at:          10.148.10.5:45721
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:    tcp://10.148.10.5:32941
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:39939
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:39939
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          10.148.10.5:45781
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:39939
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-sugjvte3
distributed.worker - INFO -                Memory:                   12.11 GB
/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://10.148.10.5:41793
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-ri99p4uk
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-0be4tfdp
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:    tcp://10.148.10.5:41793
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          10.148.10.5:34161
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-4juzbbht
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:39939
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          4
/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://10.148.10.5:43715
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-tec9d_iw
distributed.worker - INFO -          Listening to:    tcp://10.148.10.5:43715
distributed.worker - INFO -          dashboard at:          10.148.10.5:37043
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://10.148.10.15:39939
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          4
distributed.worker - INFO -                Memory:                   12.11 GB
distributed.worker - INFO -       Local Directory: /glade/u/home/apinard/ldcpy/docs/source/notebooks/dask-worker-space/worker-kvpiqc7c
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:39939
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:39939
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:39939
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:39939
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:39939
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:39939
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:39939
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:39939
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.148.10.15:39939
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.utils_perf - INFO - full garbage collection released 55.54 MB from 1362 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 195.19 MB from 2050 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 333.94 MB from 1529 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 65.76 MB from 1658 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 346.63 MB from 1539 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 54.72 MB from 1262 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 762.42 MB from 1347 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 58.26 MB from 1768 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 200.35 MB from 1320 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 200.40 MB from 1335 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 1.45 GB from 736 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 734.37 MB from 271 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 867.98 MB from 397 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 171.75 MB from 2039 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 200.40 MB from 1553 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 193.71 MB from 1587 reference cycles (threshold: 10.00 MB)
distributed.worker - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 9.90 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 9.61 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 9.74 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 62% memory usage. Resuming worker. Process memory: 7.58 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 9.70 GB -- Worker memory limit: 12.11 GB
distributed.worker - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 8.82 GB -- Worker memory limit: 12.11 GB
distributed.utils_perf - INFO - full garbage collection released 43.22 MB from 2045 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 40.01 MB from 1241 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 267.21 MB from 1408 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 49.79 MB from 1865 reference cycles (threshold: 10.00 MB)
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.10.5:38053'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.10.5:41039'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.10.5:33821'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.10.5:45853'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.10.5:45769'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.10.5:39453'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.10.5:41169'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.10.5:44119'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.10.5:42747'
/ncar/usr/jupyterhub/envs/cmip6-201910/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 54 leaked semaphores to clean up at shutdown
  len(cache))
